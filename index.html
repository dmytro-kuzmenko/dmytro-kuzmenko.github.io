<!doctype html>
<html>

<div class="page-background"></div>

<head>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<title>Dmytro Kuzmenko's webpage</title>

<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/extra_min_styles.css">

<meta name="viewport" content="width=device-width">

    <!-- Icons -->
<link rel="stylesheet" href="stylesheets/css/academicons.css"/>
<link rel="stylesheet" href="stylesheets/css/font-awesome.css"/>
<link rel="stylesheet" href="stylesheets/css/academicons.min.css"/>
<link rel="stylesheet" href="stylesheets/style-svg.html">

<!-- Meta Pixel Code -->
<script>
!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?
n.callMethod.apply(n,arguments):n.queue.push(arguments)};
if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
n.queue=[];t=b.createElement(e);t.async=!0;
t.src=v;s=b.getElementsByTagName(e)[0];
s.parentNode.insertBefore(t,s)}(window, document,'script',
'https://connect.facebook.net/en_US/fbevents.js');
fbq('init', '263777962599205');
fbq('track', 'PageView');
</script>
<noscript><img height="1" width="1" style="display:none"
src="https://www.facebook.com/tr?id=263777962599205&ev=PageView&noscript=1"
/></noscript>
<!-- End Meta Pixel Code -->

</head>

<body background="./background.jpeg" data-new-gr-c-s-check-loaded="8.876.0" data-gr-ext-installed=""><div class="wrapper">

    <!-- HEADER -->
    <header>
        <h1 align="center">Dmytro Kuzmenko</h1>

        <!-- Picture + Links + News -->
        <div style="margin-bottom: 10px" align="center">

            <!-- Portrait -->
            <!-- <img src="images/portrait.JPG" alt="Portrait" WIDTH=160 style="margin-bottom: 8px;border-radius: 10px;"> -->
            <img src="./files/photo.png" alt="Portrait" class="portrait">

            <!-- Email -->
            <!-- <div style="margin-bottom: 6px;"><img src="./files/email.png" width="175"></div> -->
            <div style="margin-bottom: 15px;"><a href = "mailto: dmytro.o.kuzmenko@gmail.com">dmytro.o.kuzmenko@gmail.com</a>

            <br>

            <!-- CV -->
<!--             <a href="./files/CV-DmytroKuzmenko.pdf" style="margin-right:14px;">
                <i class="ai ai-cv" style="font-size: 32px;"></i>
            </a> -->

            <!-- LinkedIn -->
            <a href="https://www.linkedin.com/in/righteousronin/" style="margin-right:14px;">
                <i class="fa fa-linkedin" style="font-size:32px;"></i>
            </a>

            <!-- Git -->
            <a href="https://github.com/righteousronin" style="margin-right:14px;">
                <i class="fa fa-github" style="font-size:32px;"></i>
                <!-- color: #A7A499" -->
            </a>

            <!-- Medium -->
            <a href="https://righteous-ronin.medium.com" style="margin-right:14px;">
                <i class="fa fa-medium" style="font-size:32px;"></i>
            </a>

            <!-- Twitter -->
            <a href="https://twitter.com/kuzmenko_dmytro" style="margin-right:14px;">
                <i class="fa fa-twitter" style="font-size:32px;"></i>
            </a>
            <br>
            <br>
        </div>

    </header>

    <section>

    <h1>Hello!</h1>
    <p>I am a Machine Learning Researcher and Engineer, and a postgraduate student in Applied Mathematics at the National University of Kyiv-Mohyla Academy. My research direction involves the intersection of Machine Learning, Computer Vision, and Reinforcement Learning focused on Healthcare and Intelligent Assistive Technology. My recent projects include devising deep learning architectures for various domains such as spatiotemporal, audio, and textual data. Currently, I am focusing on the Adversarial Robustness in Deep Learning models.</p>

    <p>During my undergraduate sophomore year in Software Engineering, I have discovered the world of AI and got instantly attached to it. Since then I have orchestrated full ML development pipelines and designed a couple of end-to-end solutions still used in production. I am keen on teaching and mentoring everyone interested in Machine Learning.</p>

    <p>In my free time, I read guest lectures, guide mentees, and run a <a href="https://righteous-ronin.medium.com" target="_blank" rel="noopener noreferrer">DL paper review blog</a> on Medium. I am always open to new collaborations, hit me up with an email if you want to chat!</p>

    <p>
    	Book a mentoring session with me via <a href="https://www.prjctrmentor.com/mentor/dmytro-kuzmenko" target="_blank" rel="noopener noreferrer">Projector Mentorship Platform.</a>
    </p>
    
    <h3>Recent News</h3>
    <ul>
    	<li>2022-05: Held a guest lecture at Projector Institute on the topic regression in ML. <a href="https://prjctr.com/library/video/vstup-do-regresiyi-v-mashinnomu-navchanni" target="_blank" rel="noopener noreferrer">Lecture link.</a></li>
    	<li>2022-04: Began leading a computer vision research on assistive lower limb exoskeleton in <a href="https://www.linkedin.com/posts/brokoslaw-laschowski-8a438758_ukraine-standwithukriane-ml-activity-6924755979758673920-dt4D?utm_source=linkedin_share&utm_medium=member_desktop_web" target="_blank" rel="noopener noreferrer">University of Toronto Summer Research program</a></li>
        <li>2022-03: Started an AI-centred <a href="https://t.me/aicoven" target="_blank" rel="noopener noreferrer">Telegram channel</a> and <a href="https://righteous-ronin.medium.com" target="_blank" rel="noopener noreferrer">a Medium blog.</a></li>
        <li>2022-02: Pitching <a href="https://docs.google.com/presentation/d/1vPYM-vPZQ2GFgk9oqp5px6_QXoq0ZxqH/edit?usp=sharing&ouid=102286188667012603283&rtpof=true&sd=true" target="_blank" rel="noopener noreferrer">solution</a> for building new stores network system at AI/ML Ideathon 2.</li>
        <li>2021-12: <a href="https://gdsc.community.dev/events/details/developer-student-clubs-kyiv-school-of-economics-presents-machine-learning-vs-software-engineering-overview-of-ai-foundations-and-application-areas/" target="_blank" rel="noopener noreferrer">ML-centered lecture</a> for students of Kyiv School of Economics.</li>
        <li>2021-09: <a href="https://youtu.be/M7cR-Sd4XU0" target="_blank" rel="noopener noreferrer">Pitched</a> virtual try-on and wardrobe recommender system at WILD WILD HACK 2021.</li>
        <li>2021-06: Got my Bachelors in SWE at NaUKMA.</li>
        <li>2021-05: Architectured an <a href="http://ekmair.ukma.edu.ua/bitstream/handle/123456789/22455/Kuzmenko_Bakalavrska_robota.pdf?sequence=1&isAllowed=y" target="_blank" rel="noopener noreferrer">open-data system</a> for the State Register of Court Decisions for YouControl.</li>
    </ul>

    <!-- Paper review scroll table below -->

    <h3> Highlights </h3>
    <table style="margin-top:-12px;">

    	<!-- Review -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="files/lecture.png" alt="Lecture">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://prjctr.com/library/video/vstup-do-regresiyi-v-mashinnomu-navchanni">"Intro to Regression in Machine Learning" at Projector Institute </a></b>
                    <br>
                    <i>16 May 2022</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                	Held an introductory guest lecture on the basics of regression in ML, showcased a full modelling pipeline.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody>

        <!-- Review -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/zerogen.png" alt="ZeroGEN overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/overview-zerogen-efficient-zero-shot-learning-via-dataset-generation-8ebca0c72620">Overview — ZeroGen, Efficient Zero-shot Learning via Dataset Generation </a></b>
                    <br>
                    <i>16 Feb 2022</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    An interesting take on zero-shot learning was introduced in a paper that was dated Feb 16.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody>

    	<!-- Review -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/AutoDIME.png" alt="AutoDIME overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/overview-autodime-automatic-design-of-interesting-multi-agent-environments-6ee7fd3085">Overview — AutoDIME, Automatic Design of Interesting Multi-Agent Environments </a></b>
                    <br>
                    <i>4 Mar 2022</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    The paper generalises curriculum learning for environment generation to multi-agent environments. Teacher-Student Curriculum Learning is used, where an RL-trained teacher samples environments of one or several student agents and is trained alongside the students.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody>
          
        <!-- Work -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/LiDAR-4D-DS-Net.png" alt="LiDAR overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/overview-2022-03-14-lidar-based-4d-panoptic-segmentation-via-dynamic-shifting-network-2e06733a21d7">Overview — LiDAR-based 4D Panoptic Segmentation via Dynamic Shifting Network </a></b>
                    <br>
                    <i>14 Mar 2022</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    Dynamic Shifting Network (DS-Net) is proposed, which serves as an effective panoptic segmentation framework in the point cloud realm.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody>

        <!-- Review 2 -->

        <!-- Work -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/BlockRecurrentTransformer.png" alt="Block-Recurrent Transformer overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/block-recurrent-transformer-overview-9a6e25078672"> Overview — Block-Recurrent Transformer </a></b>
                    <br>
                    <i>11 Mar 2022</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    The authors introduce the Block-Recurrent Transformer, which applies a transformer layer in a recurrent fashion along a sequence, and has linear complexity with respect to the sequence length.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody>

        <!-- Review 3 -->

        <!-- Work -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/VideoSwin.png" alt="Video Swin overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/video-swin-transformer-overvie-8fcc9b3f4f63"> Overview — Video Swin Transformer </a></b>
                    <br>
                    <i>24 Jun 2021</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    Video Swin Transformer advocates an inductive bias of locality in video Transformers, leading to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody>

        <!-- Review 4 -->

        <!-- Work -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/LipSound2.png" alt="LipSound2 overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/overview-lipsound2-self-supervised-pre-training-for-lip-to-speech-reconstruction-and-lip-reading-6a50ec3d2818"> Overview — LipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction and Lip Reading </a></b>
                    <br>
                    <i>9 Dec 2021</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    The authors propose LipSound2 which consists of an encoder-decoder architecture and location-aware attention mechanism to map face image sequences to mel-scale spectrograms directly without requiring any human annotations.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody></table>

    <!-- </table> -->
    </section> 

<footer>
    <br>
    <p style="line-height: 1.2;" align="center"><small>Website is based</a> on <a href="https://github.com/orderedlist/minimal">minimal</a>  <br>and inspired by David Abel's  <a href="https://david-abel.github.io/">website</a> <br>
        &copy; Dmytro Kuzmenko 2022 </small></p>
</footer>
</div>
<script src="javascripts/scale.fix.js"></script>
</body>	
</html>
