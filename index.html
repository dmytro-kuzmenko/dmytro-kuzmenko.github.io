<!doctype html>
<html>
<head>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<title>Dmytro Kuzmenko's webpage</title>

<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/extra_min_styles.css">

<meta name="viewport" content="width=device-width">

    <!-- Icons -->
<link rel="stylesheet" href="stylesheets/css/academicons.css"/>
<link rel="stylesheet" href="stylesheets/css/font-awesome.css"/>
<link rel="stylesheet" href="stylesheets/css/academicons.min.css"/>
<link rel="stylesheet" href="stylesheets/style-svg.html">

<!-- Meta Pixel Code -->
<script>
!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?
n.callMethod.apply(n,arguments):n.queue.push(arguments)};
if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
n.queue=[];t=b.createElement(e);t.async=!0;
t.src=v;s=b.getElementsByTagName(e)[0];
s.parentNode.insertBefore(t,s)}(window, document,'script',
'https://connect.facebook.net/en_US/fbevents.js');
fbq('init', '263777962599205');
fbq('track', 'PageView');
</script>
<noscript><img height="1" width="1" style="display:none"
src="https://www.facebook.com/tr?id=263777962599205&ev=PageView&noscript=1"
/></noscript>
<!-- End Meta Pixel Code -->

</head>

<body data-new-gr-c-s-check-loaded="8.876.0" data-gr-ext-installed=""><div class="wrapper">

	<style>
		body {
			background-image: url('./background.jpeg');
			background-repeat: no-repeat;
		}
	</style>

    <!-- HEADER -->
    <header>
        <h1 align="center">Dmytro Kuzmenko</h1>

        <!-- Picture + Links + News -->
        <div style="margin-bottom: 10px" align="center">

            <!-- Portrait -->
            <!-- <img src="images/portrait.JPG" alt="Portrait" WIDTH=160 style="margin-bottom: 8px;border-radius: 10px;"> -->
            <img src="./files/photo.png" alt="Portrait" class="portrait">

            <!-- Email -->
            <!-- <div style="margin-bottom: 6px;"><img src="./files/email.png" width="175"></div> -->
            <div style="margin-bottom: 15px;"><a href = "mailto: dmytro.o.kuzmenko@gmail.com">dmytro.o.kuzmenko@gmail.com</a>

            <br>

            <!-- CV -->
<!--             <a href="./files/CV-DmytroKuzmenko.pdf" style="margin-right:14px;">
                <i class="ai ai-cv" style="font-size: 32px;"></i>
            </a> -->

            <!-- LinkedIn -->
            <a href="https://www.linkedin.com/in/righteousronin/" style="margin-right:14px;">
                <i class="fa fa-linkedin" style="font-size:32px;"></i>
            </a>

            <!-- Git -->
            <a href="https://github.com/righteousronin" style="margin-right:14px;">
                <i class="fa fa-github" style="font-size:32px;"></i>
                <!-- color: #A7A499" -->
            </a>

            <!-- Medium -->
            <a href="https://righteous-ronin.medium.com" style="margin-right:14px;">
                <i class="fa fa-medium" style="font-size:32px;"></i>
            </a>

            <!-- Twitter -->
            <a href="https://twitter.com/kuzmenko_dmytro" style="margin-right:14px;">
                <i class="fa fa-twitter" style="font-size:32px;"></i>
            </a>
            <br>
            <br>
        </div>

    </header>

    <section>

    <h1>Hello!</h1>
    <p>I am a Machine Learning Research Engineer and a postgraduate student in Applied Mathematics at the National University of Kyiv-Mohyla Academy. My research direction involves the intersection of Machine Learning (ML), Computer Vision, and Reinforcement Learning focused on Healthcare and Intelligent Assistive Technology. My recent projects include devising deep learning architectures for various domains such as spatiotemporal, audio, and textual data. Currently, I am focusing on the Adversarial Robustness in Deep Learning models.</p>

    <p>During my undergraduate sophomore year in Software Engineering, I have discovered the world of AI and got instantly attached to it. Since then I have orchestrated full ML development pipelines and designed a couple of end-to-end solutions still used in production. I am keen on teaching and mentoring everyone interested in Machine Learning.</p>

    <p>In my free time, I read guest lectures, guide mentees, and run a <a href="https://righteous-ronin.medium.com" target="_blank" rel="noopener noreferrer">DL paper review blog</a> on Medium. I am always open for new collaborations, hit me up with an email if you want to chat!</p>
    
    <h3>Recent News</h3>
    <ul>
        <li>2022-03: Started an AI-centred <a href="https://t.me/aicoven" target="_blank" rel="noopener noreferrer">Telegram channel</a> and <a href="https://righteous-ronin.medium.com" target="_blank" rel="noopener noreferrer">a Medium blog.</a></li>
        <li>2022-02: Pitching <a href="https://docs.google.com/presentation/d/1vPYM-vPZQ2GFgk9oqp5px6_QXoq0ZxqH/edit?usp=sharing&ouid=102286188667012603283&rtpof=true&sd=true" target="_blank" rel="noopener noreferrer">solution</a> for building new stores network system at AI/ML Ideathon 2.</li>
        <li>2021-12: <a href="https://gdsc.community.dev/events/details/developer-student-clubs-kyiv-school-of-economics-presents-machine-learning-vs-software-engineering-overview-of-ai-foundations-and-application-areas/" target="_blank" rel="noopener noreferrer">ML-centered lecture</a> for students of Kyiv School of Economics.</li>
        <li>2021-09: <a href="https://youtu.be/M7cR-Sd4XU0" target="_blank" rel="noopener noreferrer">Pitched</a> virtual try-on and wardrobe recommender system at WILD WILD HACK 2021.</li>
        <li>2021-06: Got my Bachelors in SWE from NaUKMA.</li>
        <li>2021-05: Architectured an <a href="http://ekmair.ukma.edu.ua/bitstream/handle/123456789/22455/Kuzmenko_Bakalavrska_robota.pdf?sequence=1&isAllowed=y" target="_blank" rel="noopener noreferrer">open-data system</a> for the State Register of Court Decisions for YouControl.</li>
    </ul>

    <!-- Paper review scroll table below -->

    <h3> Paper Reviews </h3>
    <table style="margin-top:-12px;">
          
        <!-- Work -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/LiDAR-4D-DS-Net.png" alt="LiDAR overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/overview-2022-03-14-lidar-based-4d-panoptic-segmentation-via-dynamic-shifting-network-2e06733a21d7">Overview — LiDAR-based 4D Panoptic Segmentation via Dynamic Shifting Network </a></b>
                    <br>
                    <i>14 Mar 2022</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    Dynamic Shifting Network (DS-Net) is proposed, which serves as an effective panoptic segmentation framework in the point cloud realm.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody>

        <!-- Review 2 -->

        <!-- Work -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/BlockRecurrentTransformer.png" alt="Block-Recurrent Transformer overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/block-recurrent-transformer-overview-9a6e25078672"> Overview — Block-Recurrent Transformer </a></b>
                    <br>
                    <i>11 Mar 2022</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    The authors introduce the Block-Recurrent Transformer, which applies a transformer layer in a recurrent fashion along a sequence, and has linear complexity with respect to the sequence length.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody>

        <!-- Review 3 -->

        <!-- Work -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/VideoSwin.png" alt="Video Swin overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/video-swin-transformer-overvie-8fcc9b3f4f63"> Overview — Video Swin Transformer </a></b>
                    <br>
                    <i>24 Jun 2021</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    Video Swin Transformer advocates an inductive bias of locality in video Transformers, leading to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody>

        <!-- Review 4 -->

        <!-- Work -->
        <tbody><tr style="border-bottom:1px solid #e5e5e5;">
            <!-- IMAGE -->
            <td>
                <img class="workPicture" src="projects/LipSound2.png" alt="LipSound2 overview">
            </td>

            <!-- TITLE AND INFO -->
            <td>
                <br>
                <div id="indexWorkText">
                    <!-- Title + Link -->
                    <b><a href="https://righteous-ronin.medium.com/overview-lipsound2-self-supervised-pre-training-for-lip-to-speech-reconstruction-and-lip-reading-6a50ec3d2818"> Overview — LipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction and Lip Reading </a></b>
                    <br>
                    <i>9 Dec 2021</i>
                </div>
                <br>
                <!-- One sentence description -->
                <div id="indexWorkText">
                    The authors propose LipSound2 which consists of an encoder-decoder architecture and location-aware attention mechanism to map face image sequences to mel-scale spectrograms directly without requiring any human annotations.
                </div>
                <br>
                <br>
            </td>
        </tr>     
        
        </tbody></table>

    <!-- </table> -->
    </section> 

<footer>
    <br>
    <p style="line-height: 1.2;" align="center"><small>Website is based</a> on <a href="https://github.com/orderedlist/minimal">minimal</a>  <br>and inspired by David Abel's  <a href="https://david-abel.github.io/">website</a> <br>
        &copy; Dmytro Kuzmenko 2022 </small></p>
</footer>
</div>
<script src="javascripts/scale.fix.js"></script>
</body>	
</html>
